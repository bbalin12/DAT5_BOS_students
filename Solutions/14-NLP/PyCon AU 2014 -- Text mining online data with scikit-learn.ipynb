{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:ecc28b275b5a78d216d966f0441737d608760fcd367619e86a576af5e7c3e8fa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"files/Cloud2.png\" width=600px>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Text mining online data with scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Text mining has a large variety of applications and is becoming used in more businesses for gathering intelligence and providing insight. People are sending text constantly online via social media, chat rooms and blogs. Tapping into this information can help businesses gain an advantage and is increasingly a necessary skill for data analytics. Text mining is a unique data mining problem, dealing with real world data that is often heavy on artefacts, difficult to model and challenging to properly manage. Text mining can be seen as a bit of a dark art that is difficult to learn and gain traction. However some basic strategies can often be applied to get good results quite quickly, and the same basic models appear in many text mining challenges.\n",
      "\n",
      "The scikit-learn project is a library of machine learning algorithms for the scientific python stack (numpy & scipy). It is known for having detailed documentation, a high quality of coding and a growing list of users worldwide. The documentation includes tutorials for learning machine learning as well as the library and is a great place to start for beginners wanting to learn data analytics. There is a strong focus on reusable components and useful algorithms, and the text mining sections of scikit-learn follow the \u201cstandard model\u201d of text mining quite well.\n",
      "\n",
      "In this presentation, we will go through the scikit-learn project for machine learning and show how to use it for text mining applications. Real world data and applications will be used, including spam detection on Twitter, predicting the author of a program and determining a user's political bent based on their social media account."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Who am I"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Robert Layton\n",
      "\n",
      "* Research Fellow at Federation University Australia\n",
      "* Data analyst, lots of text\n",
      "* scikit-learn contributor (including GSoC mentor)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "print(\"Python: {}\".format(sys.version))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Python: 3.4.0 (default, Apr 11 2014, 13:05:11) \n",
        "[GCC 4.8.2]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import sklearn\n",
      "print(\"scikit-learn: {}\".format(sklearn.__version__))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "scikit-learn: 0.15.0\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What this talk is about"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Using scikit-learn for text mining.\n",
      "\n",
      "* I'll go through a standard category example\n",
      "* Then doing spam detection on Twitter\n",
      "\n",
      "Then I'll quickly talk about some other options and how they would integrate into scikit-learn's framework."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What this talk is not!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This isn't a talk about:\n",
      "\n",
      "* NLTK -- A great library, but I'm trying to be focused\n",
      "* Pandas -- Again, great, but out of scope\n",
      "* Obtaining online data -- maybe another year!\n",
      "* Ethics -- for this talk, just assume we got consent\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's get some data\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "newsgroups_train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'soc.religion.christian', 'talk.politics.guns'])\n",
      "\n",
      "from pprint import pprint\n",
      "pprint(list(newsgroups_train.target_names))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['alt.atheism', 'soc.religion.christian', 'talk.politics.guns']\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's split the dataset into two sets: training and testing\n",
      "from sklearn.cross_validation import train_test_split\n",
      "docs_train, docs_test, y_train, y_test = train_test_split(newsgroups_train.data, newsgroups_train.target)\n",
      "print(\"Number of training documents: {}\".format(len(docs_train)))\n",
      "print(\"Number of testing documents: {}\".format(len(docs_test)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of training documents: 1218\n",
        "Number of testing documents: 407\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First: how are we going to evaluate?\n",
      "# F-score -- related to accuracy, based on precision and recall\n",
      "# Hard to \"fake\" for unbalanced datasets\n",
      "from sklearn.metrics import f1_score"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "A basic example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Machine learning algorithms don't typically take text as input.\n",
      "\n",
      "Instead, we need to convert the text into a vector:\n",
      "\n",
      "$x_i = [x_{i,0}, x_{i,1}, x_{i,2}, ... ]$\n",
      "\n",
      "We can do that easily by simply choosing a list of words and counting how frequently they occur:\n",
      "\n",
      "$x_{i, j}$ is the frequency of word $j$ in document $i$.\n",
      "\n",
      "We can manually choose our list of words, or we can set it from the data. Usually we set from the data, for example \"The least frequently occuring words\". This is called the \"bag of words\" model, and scikit-learn has is built-in:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# Fit on training data\n",
      "model = CountVectorizer()\n",
      "X_train = model.fit_transform(docs_train)\n",
      "\n",
      "# Vocabulary is the words used, and is a dict, available in model.vocabulary_\n",
      "# They map the words to the indices\n",
      "pprint(list(model.vocabulary_.items())[:10])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('outgrowth', 15427),\n",
        " ('lpf', 13270),\n",
        " ('genocide', 9865),\n",
        " ('san', 18510),\n",
        " ('renouncing', 17802),\n",
        " ('mothers', 14301),\n",
        " ('swimmer', 20446),\n",
        " ('fifth', 9195),\n",
        " ('buyer', 4573),\n",
        " ('manhattan', 13482)]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# X_train gives us our bag of words matrix: X_train[i][j] is the value of word with index j for document with index i\n",
      "# It is a sparse matrix, which we will get to later on\n",
      "print(type(X_train))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'scipy.sparse.csr.csr_matrix'>\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "keyword = \"believe\"\n",
      "documents_containing_keyword = [index for index in range(len(docs_train)) if keyword in docs_train[index]]\n",
      "assert len(documents_containing_keyword) > 0\n",
      "keyword_index = model.vocabulary_[keyword]\n",
      "document_index = documents_containing_keyword[0]\n",
      "print(keyword_index in X_train[document_index].nonzero()[1])\n",
      "print(\"The keyword {} appears {} times in document {}\".format(keyword, X_train[document_index,keyword_index], document_index))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "The keyword believe appears 1 times in document 12\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's compare some words, and how they differ between categories\n",
      "words = [\"believe\", \"right\", \"bible\"]\n",
      "word_indices = [model.vocabulary_[word] for word in words]\n",
      "classes = sorted(set(y_train))\n",
      "categories = newsgroups_train.target_names\n",
      "#print(X_train[y_train == 0, word_indices[0]])\n",
      "frequency = np.array([[X_train[y_train == category,wi].mean() for wi in word_indices]\n",
      "                       for category in classes]).T\n",
      "\n",
      "assert frequency.shape == (len(word_indices), sum(set(y_train))), frequency.shape\n",
      "print(frequency)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.53351955  0.702407    0.37717122]\n",
        " [ 0.32960894  0.26039387  0.56823821]\n",
        " [ 0.34078212  0.64989059  0.00496278]]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "# Setup the plot\n",
      "from matplotlib import pyplot as plt\n",
      "ind = np.arange(frequency.shape[0])\n",
      "width = 0.2\n",
      "\n",
      "colors = \"rgbyck\"\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=(20, 8))\n",
      "ax = fig.add_subplot(111)\n",
      "for column in range(frequency.shape[1]):\n",
      "    ax.barh(ind + (width * column), frequency[:,column], width, color=colors[column], label=categories[column])\n",
      "ax.set(yticks=ind + width, yticklabels=words, ylim=[len(words)*width - 1, frequency.shape[0]])\n",
      "ax.legend(bbox_to_anchor=(0.9, 0.8))\n",
      "r = plt.xlim((0, 1))\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAABJgAAAHaCAYAAABM/PNwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nt6QAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f24b3c7ff98>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can use our existing model to transform the test documents in the same way\n",
      "# Because we don't fit again, the indices match with the previouc\n",
      "X_test = model.transform(docs_test)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Then we build a basic classifier and test it out\n",
      "from sklearn.svm import SVC\n",
      "clf = SVC().fit(X_train, y_train)\n",
      "y_pred = clf.predict(X_test)\n",
      "print(\"F1-score: {:.3f}\".format(f1_score(y_test, y_pred)))\n",
      "print(\"Accuracy: {:.3f}\".format(np.mean(y_test == y_pred)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F1-score: 0.527\n",
        "Accuracy: 0.597\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python3/dist-packages/scipy/sparse/compressed.py:119: UserWarning: indptr array has non-integer dtype (float64)\n",
        "  % self.indptr.dtype.name)\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's put that all into a short snippet:\n",
      "text_model = CountVectorizer()\n",
      "clf_model = SVC()\n",
      "\n",
      "# Convert documents to vectors\n",
      "X_train = text_model.fit_transform(docs_train)\n",
      "X_test = text_model.transform(docs_test)\n",
      "\n",
      "# Train classifier\n",
      "clf_model.fit(X_train, y_train)\n",
      "y_pred = clf_model.predict(X_test)\n",
      "\n",
      "# Evaluate\n",
      "print(\"F1-score: {:.3f}\".format(f1_score(y_test, y_pred)))\n",
      "print(\"Accuracy: {:.3f}\".format(np.mean(y_test == y_pred)))\n",
      "# The results will change, as there is some randomness.\n",
      "# We can usually address that using random_state, but that is out of scope for today.\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F1-score: 0.527\n",
        "Accuracy: 0.597\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Spam Detection on Twitter"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![Word Cloud](files/Cloud3.png \"Word Cloud\")\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "\n",
      "Spam on social media is a big problem -- it ruins the user experience for people, wastes resources for companies and is just generally annoying. It can also propogate crime, but allowing criminals to advertise their goods without having to go through other channels. Luckily, the same techniques as above can be applied here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get a dataset of spam and non-spam twitter posts\n",
      "# Offline, I collected a bunch of twitter posts and manually sorted through a few thousand.\n",
      "# This is a pretty easy job for a human, but can be hard for machines!\n",
      "import numpy as np\n",
      "\n",
      "def load_twitter_data(filename='tweets_spam.csv'):\n",
      "    documents = []\n",
      "    classes = []\n",
      "    with open(filename) as inf:\n",
      "        for line in inf:\n",
      "            data = line.split(\",\")\n",
      "            documents.append(\",\".join(data[:-1]))\n",
      "            classes.append(int(data[-1]))\n",
      "    classes = np.array(classes, dtype='int')\n",
      "    return documents, classes"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents, classes = load_twitter_data()\n",
      "print(\"Loaded {} documents, {} are spam and {} are not\".format(len(documents), sum(classes == 1), sum(classes == 0)))\n",
      "docs_train, docs_test, y_train, y_test = train_test_split(documents, classes)\n",
      "print(\"Number of training documents: {}\".format(len(docs_train)))\n",
      "print(\"Number of testing documents: {}\".format(len(docs_test)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded 502 documents, 104 are spam and 398 are not\n",
        "Number of training documents: 376\n",
        "Number of testing documents: 126\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's use our previous model and see how that goes...\n",
      "\n",
      "text_model = CountVectorizer()\n",
      "clf_model = SVC()\n",
      "\n",
      "# Convert documents to vectors\n",
      "X_train = text_model.fit_transform(docs_train)\n",
      "X_test = text_model.transform(docs_test)\n",
      "\n",
      "# Train classifier\n",
      "clf_model.fit(X_train, y_train)\n",
      "y_pred = clf_model.predict(X_test)\n",
      "\n",
      "# Evaluate\n",
      "print(\"F1-score: {:.3f}\".format(f1_score(y_test, y_pred)))\n",
      "print(\"Accuracy: {:.3f}\".format(np.mean(y_test == y_pred)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F1-score: 0.000\n",
        "Accuracy: 0.786\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# That's odd -- high accuracy, (very) low f-score\n",
      "# A confusion matrix will show us the issue here\n",
      "from sklearn.metrics import classification_report\n",
      "print(classification_report(y_test, y_pred))\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.79      1.00      0.88        99\n",
        "          1       0.00      0.00      0.00        27\n",
        "\n",
        "avg / total       0.62      0.79      0.69       126\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/metrics.py:1771: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
        "  'precision', 'predicted', average, warn_for)\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "In other words, the classifier is \"cheating\" by just saying all entries are not spam. This is very accurate, but only because most of our dataset is not spam.\n",
      "Instead, let's try a bunch of different parameters.\n",
      "\n",
      "We can build a pipeline to handle this for us, and that let's us try different parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import Pipeline\n",
      "text_model = CountVectorizer()\n",
      "clf_model = SVC()\n",
      "pipeline = Pipeline([('vectorizer', text_model),\n",
      "                     ('classifier', clf_model)\n",
      "                      ])\n",
      "# With the pipeline define, we set out parameters in a dictionary, which we can specify as ranges\n",
      "params = {\n",
      "            # Vectoriser parameters\n",
      "            'vectorizer__ngram_range': [(1,3), ],  # n-grams are subsequences of \"tokens\"\n",
      "            'vectorizer__analyzer': ['word',],  # words are our tokens\n",
      "            'vectorizer__min_df': [2, 3],  # n-grams need to appear in at least this many documents in the dataset\n",
      "            # Classifier parameters\n",
      "            'classifier__C': [0.1, 1.0, 10, ],  # See Support Vector Machines information\n",
      "            'classifier__kernel': ['rbf', 'linear'],\n",
      "           }"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "But... how do we choose out parameters? We can't use our existing test set, due to overfitting. Overfitting is the effect of training a model too specifically.\n",
      "\n",
      "Typically this is handled by performing cross fold validation, which scikit-learn supports for pipelines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.cross_validation import StratifiedKFold\n",
      "grid = GridSearchCV(\n",
      "                   pipeline,  # Our pipeline from above\n",
      "                   params,  # The parameters we set above\n",
      "                   refit=True,  # Ignore me for now\n",
      "                   n_jobs=1,  # We can set this to the number of cores to use, or -1 for \"all\"\n",
      "                   scoring='f1',  # f1 score\n",
      "                   cv=StratifiedKFold(y_train, n_folds=3),  # What type of cross fold validation to use\n",
      "                )"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now, we fit out model\n",
      "grid.fit(docs_train, y_train)\n",
      "\n",
      "# and then evaluate!\n",
      "y_pred = grid.predict(docs_test)\n",
      "\n",
      "# Evaluate\n",
      "print(\"F1-score: {:.3f}\".format(f1_score(y_test, y_pred)))\n",
      "print(\"Accuracy: {:.3f}\".format(np.mean(y_test == y_pred)))\n",
      "print(classification_report(y_test, y_pred))\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "F1-score: 0.681\n",
        "Accuracy: 0.881\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.90      0.96      0.93        99\n",
        "          1       0.80      0.59      0.68        27\n",
        "\n",
        "avg / total       0.88      0.88      0.87       126\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "By running a bunch of parameters, we can find a model using training data that better represents our data.\n",
      "The above report can be roughly summarised as:\n",
      "\n",
      "* When we predicted something as spam, we were right 80% of the time\n",
      "* Of the things that are spam, we found 59% of them\n",
      "\n",
      "So not bad!\n",
      "\n",
      "From here, you need to define what your evaluation really means. If you are building a spam detector for twitter, you may not care about missing legitimate content -- in this case, you can use an evaluate measure that finds more spam, but with more \"false positives\". If you are building one for your work email, you probably don't want to miss that email from your boss. For this reason, you would penalise \"false positives\" more, leading to more spam in your inbox, but less legitimate emails missed.\n",
      "\n",
      "One of the key parameters here was the n-gram, which allows us to choose several words in a row as a feature. For twitter spam, often \"normal\" words are used, so looking at single words is unlikely to give us a good result -- they occur in normal and spam tweets with approximately the same frequency."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Other possible applications"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Authorship analysis is a growing field aiming to recognised the author of a document, using only the content of that document.\n",
      "Typically this is performed using natural language documents (I covered this briefly last year).\n",
      "\n",
      "However, it works for software too! Using the same types of features, you can often predict which of a set of authors wrote a given document.\n",
      "\n",
      "We can do this by looking at character n-grams instead of word n-grams."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We can use our previous pipeline, but will use characters instead of words as our tokens\n",
      "# With the pipeline define, we set out parameters in a dictionary, which we can specify as ranges\n",
      "params = {\n",
      "            # Vectoriser parameters\n",
      "            'vectorizer__ngram_range': [(1,3), ],  # n-grams are subsequences of \"tokens\"\n",
      "            'vectorizer__analyzer': ['char',],  # characters are our tokens\n",
      "            'vectorizer__min_df': [2, 3],  # n-grams need to appear in at least this many documents in the dataset\n",
      "            # Classifier parameters\n",
      "            'classifier__C': [0.1, 1.0, 10, ],  # See Support Vector Machines information\n",
      "            'classifier__kernel': ['rbf', 'linear'],\n",
      "           }"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Other applications from research include applying LIWC to predicting a social media's political stance.\n",
      "\n",
      "See: Tumasjan, A., Sprenger, T. O., Sandner, P. G., & Welpe, I. M. (2010). Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment. ICWSM, 10, 178-185.\n",
      "\n",
      "LIWC enhances word vectors with information about the contexts in which they are used -- a bit like semantics on steroids."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Thanks!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can reach me at robertlayton@gmail.com\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}